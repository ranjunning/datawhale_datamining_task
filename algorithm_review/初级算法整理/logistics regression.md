### 1、逻辑回归与线性回归的联系与区别

线性回归用来预测，逻辑回归用来分类；
线性回归是拟合函数，逻辑回归是预测函数；
线性回归的参数计算方法是最小二乘法，逻辑回归的参数计算方法是梯度下降

### 2. 逻辑回归的原理

逻辑回归的原理，算法最本质的核心在于sigmod函数，在线性回归的模型框架下外嵌一个sigmod函数，其本质的原理是对于线性回归的连续性数据进行离散化，局限于sigmod函数的本身，对于逻辑回归也存在一些修改的模型

### 3. 逻辑回归损失函数推导及优化

![1565356491486](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\1565356491486.png)

最大似然估计：

![1565356512729](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\1565356512729.png)

### 4. 正则化与模型评估指标

正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项（regularizer）或罚项（penalty term）。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。比如正则化项可以是模型参数向量的范数。

![1565356549188](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\1565356549188.png)

### 5. 逻辑回归的优缺点

- 优点：

  1. 简单
  2. 模型效果不错，在工程上是可接受的（作baseline），主要取决于特征工程的建立
  3. 训练速度快
  4. 资源占用小
  5. 如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。

  缺点：

  

  1. 准确率并不是很高。

  2. 很难处理数据不平衡的问题。

  3. 处理非线性数据较麻烦。

  4. 逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。`

     ### 6. 样本不均衡问题解决办法

     扩大数据集
     尝试其它评价指标 F1/ReCall/Kappa/ROC
     重采样
     其他分类模型
     增加惩罚项

     #### 7. sklearn参数

