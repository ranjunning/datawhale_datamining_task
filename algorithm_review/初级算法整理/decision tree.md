### 信息论基础


主要概念：
熵：熵可以看作是随机变量的平均不确定度的度量。在平均意义下，它是为了描述该随机变量所需的比特数。
联和熵：联合熵就是度量一个联合分布的随机系统的不确定度
条件熵：表示在已知随机变量 X 的条件下随机变量 Y 的不确定性
信息增熵：在一个条件下，信息不确定性减少的程度
基尼不纯度：将来自集合中的某种结果随机应用于集合中某一数据项的预期误差率。

### 决策树的不同分类算法

基于信息论的三种决策树算法有ID3，C4.5，CART。
ID3算法由Ross Quinlan发明，建立在“奥卡姆剃刀”的基础上：越是小型的决策树越优于大的决策树（be simple简单理论）。ID3算法中根据信息论的信息增益评估和选择特征，每次选择信息增益最大的特征做判断模块。ID3算法可用于划分标称型数据集，没有剪枝的过程，为了去除过度数据匹配的问题，可通过裁剪合并相邻的无法产生大量信息增益的叶子节点（例如设置信息增益阀值）。使用信息增益的话其实是有一个缺点，那就是它偏向于具有大量值的属性–就是说在训练集中，某个属性所取的不同值的个数越多，那么越有可能拿它来作为分裂属性，而这样做有时候是没有意义的，另外ID3不能处理连续分布的数据特征，于是就有了C4.5算法。CART算法也支持连续分布的数据特征。
C4.5是ID3的一个改进算法，继承了ID3算法的优点。C4.5算法用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足在树构造过程中进行剪枝；能够完成对连续属性的离散化处理；能够对不完整数据进行处理。C4.5算法产生的分类规则易于理解、准确率较高；但效率低，因树构造过程中，需要对数据集进行多次的顺序扫描和排序。也是因为必须多次数据集扫描，C4.5只适合于能够驻留于内存的数据集。
CART算法的全称是Classification And Regression Tree，采用的是Gini指数（选Gini指数最小的特征s）作为分裂标准,同时它也是包含后剪枝操作。ID3算法和C4.5算法虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但其生成的决策树分支较大，规模较大。为了简化决策树的规模，提高生成决策树的效率，就出现了根据GINI系数来选择测试属性的决策树算法CART。

### 回归树原理

回归树与决策树相同，只是在分类树中，采用信息论中的方法，通过计算选择最佳划分点。而在回归树中，采用的是启发式的方法。

### 决策树防止过拟合手段

决策树过拟合有两方面的原因，分别是数据问题以及建模问题。
针对数据部分解决过拟合的手段是：做好数据预处理工作，合理选择数据样本，用相对能够反映业务逻辑的训练集去产生决策树；
针对建模过程解决的手段是：剪枝

### 模型评估

前面已经介绍过模型评估的主要方法，在此处主要使用模型验证和准确率判定的方法进行评估验证。相见sklearn中的sklearn.metrics.precision_score方法

### sklearn参数详解，Python绘制决策树

sklearn中参数详解参考如下：
决策树参数详解
使用Python绘制决策树代码如下：

tree.export_graphviz(clf,out_file = dot_data,feature_names=feature_name,
                     class_names=target_name,filled=True,rounded=True,
                     special_characters=True)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())

